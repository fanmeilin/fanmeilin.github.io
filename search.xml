<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Mahalanobis_distance</title>
      <link href="/2021/08/09/Mahalanobis-distance/"/>
      <url>/2021/08/09/Mahalanobis-distance/</url>
      
        <content type="html"><![CDATA[<blockquote><p>马氏距离(Mahalanobis Distance)是度量学习中一种常用的距离指标，同欧氏距离、曼哈顿距离、汉明距离等一样被用作评定数据之间的相似度指标。但却可以应对高维线性分布的数据中各维度间非独立同分布的问题。</p></blockquote><p><em>使用马氏距离，对高维非独立分布的数据进行距离度量。</em></p><p><strong>那我们为什么要用马氏距离呢？</strong><br>马氏距离有很多<strong>优点：</strong> <strong>马氏距离不受量纲的影响</strong>，两点之间的马氏距离与原始数据的测量单位无关；由标准化数据和中心化数据(即原始数据与均值之差）计算出的二点之间的马氏距离相同。<strong>马氏距离还可以排除变量之间的相关性的干扰</strong>。</p><h2 id="什么是马氏距离"><a href="#什么是马氏距离" class="headerlink" title="什么是马氏距离"></a>什么是马氏距离</h2><p>马氏距离(Mahalanobis Distance)是一种距离的度量，可以看作是欧氏距离的一种修正，修正了欧式距离中各个维度尺度不一致且相关的问题。</p><p>单个数据点的马氏距离</p><p><img src="https://pic4.zhimg.com/80/v2-d2987369d8167a362482d6cbecefb8bb_720w.jpg"></p><p>数据点x, y之间的马氏距离</p><p><img src="https://pic3.zhimg.com/80/v2-d54956df14c05568f8c0c0548ac16416_720w.jpg"></p><p><em>其中Σ是多维随机变量的协方差矩阵，μ为样本均值，如果协方差矩阵是单位向量，也就是各维度独立同分布，马氏距离就变成了欧氏距离。</em></p><h2 id="马氏距离实际意义"><a href="#马氏距离实际意义" class="headerlink" title="马氏距离实际意义"></a>马氏距离实际意义</h2><p>那么马氏距离就能能干什么？它比欧氏距离好在哪里？举几个栗子</p><p><strong>欧式距离近就一定相似？</strong></p><p>先举个比较常用的例子，身高和体重，这两个变量拥有不同的单位标准，也就是有不同的scale。比如身高用毫米计算，而体重用千克计算，显然差10mm的身高与差10kg的体重是完全不同的。但在普通的欧氏距离中，这将会算作相同的差距。</p><p><strong>归一化后欧氏距离近就一定相似？</strong></p><p>当然我们可以先做归一化来消除这种维度间scale不同的问题，但是样本分布也会影响分类</p><p>举个一维的栗子，现在有两个类别，统一单位，第一个类别均值为0，方差为0.1，第二个类别均值为5，方差为5。那么一个值为2的点属于第一类的概率大还是第二类的概率大？距离上说应该是第一类，但是直觉上显然是第二类，因为第一类不太可能到达2这个位置。</p><p>所以，在一个方差较小的维度下很小的差别就有可能成为离群点。就像下图一样，A与B相对于原点的距离是相同的。但是由于样本总体沿着横轴分布，所以B点更有可能是这个样本中的点，而A则更有可能是离群点。</p><p><img src="https://pic4.zhimg.com/80/v2-6f5d1b59fd1687cfeecd0c6991c6db77_720w.jpg"></p><p><strong>算上维度的方差就够了？</strong></p><p>还有一个问题——如果维度间不独立同分布，样本点一定与欧氏距离近的样本点同类的概率更大吗？</p><p><img src="https://pic3.zhimg.com/80/v2-3cee35b79d272dda86e2604c160934ee_720w.jpg"></p><p>可以看到样本基本服从f(x) = x的线性分布，A与B相对于原点的距离依旧相等，显然A更像是一个离群点</p><p>即使数据已经经过了标准化，也不会改变AB与原点间距离大小的相互关系。所以要本质上解决这个问题，就要针对<a href="https://link.zhihu.com/?target=https://www.ph0en1x.space/2018/03/06/PCA/">主成分分析</a>中的<code>主成分</code>来进行标准化。</p><h2 id="马氏距离的几何意义"><a href="#马氏距离的几何意义" class="headerlink" title="马氏距离的几何意义"></a>马氏距离的几何意义</h2><p>上面搞懂了，马氏距离就好理解了，<u>只需要将变量<code>按照主成分进行旋转</code>，让维度间相互<strong>独立</strong>，然后进行<code>标准化</code></u>，让维度<strong>同分布</strong>就可以了。</p><p>由主成分分析可知，由于主成分就是特征向量方向，每个方向的方差就是对应的特征值，所以只需要按照特征向量的方向旋转，然后缩放特征值倍就可以了，可以得到以下的结果：</p><p><img src="https://pic3.zhimg.com/80/v2-068306ff7e62b7af24b126eafe0b8bc6_720w.jpg"></p><p>离群点就被成功分离，这时候的欧式距离就是马氏距离。</p><h2 id="马氏距离的推导"><a href="#马氏距离的推导" class="headerlink" title="马氏距离的推导"></a>马氏距离的推导</h2><p>首先要对数据点进行<em>旋转</em>，旋转至主成分，维度间线性无关，假设新的坐标为</p><p><img src="https://pic2.zhimg.com/80/v2-e924839926a256cb277a8cfc850d5a89_720w.jpg"></p><p>又变换后<em>维度间线性无关且每个维度自己的方差为特征值</em>，所以满足：</p><p><img src="https://pic1.zhimg.com/80/v2-24ace781a1f0b2cc64ea359b1bb78d74_720w.jpg"></p><p>马氏距离是旋转变换缩放之后的欧式距离，所以马氏距离的计算公式为：</p><p><img src="https://pic3.zhimg.com/80/v2-4435a733478fafe47ee0198e315e67f6_720w.jpg"></p><p>这就是之前提到的马氏距离的公式</p><h2 id="马氏距离的问题"><a href="#马氏距离的问题" class="headerlink" title="马氏距离的问题"></a>马氏距离的问题</h2><ul><li>协方差矩阵必须满秩</li></ul><p>里面有求逆矩阵的过程，不满秩不行，要求数据要有原维度个特征值，如果没有可以考虑先进行PCA，这种情况下PCA不会损失信息</p><ul><li>不能处理非线性流形(manifold)上的问题</li></ul><p>只对线性空间有效，如果要处理流形，只能在局部定义，可以用来建立KNN图</p><blockquote><p>参考：</p><p><a href="https://zhuanlan.zhihu.com/p/46626607">https://zhuanlan.zhihu.com/p/46626607</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> math </category>
          
          <category> 概率论 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 马氏距离 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Eckark_young定理</title>
      <link href="/2021/08/09/Eckark-young/"/>
      <url>/2021/08/09/Eckark-young/</url>
      
        <content type="html"><![CDATA[<p>最佳低秩逼近和奇异值的关系(<em>Eckart</em>-<em>Young定理</em>)</p><h3 id="定理"><a href="#定理" class="headerlink" title="定理"></a>定理</h3><p>Suppose a matrix $A\in \mathbb{R}^{m\times n}$has an SVD-decomposition$A=U\Sigma V^T$. Let $k &lt; r= \mathsf{rank}(A)$and truncated matrix<br>$$<br>A_k = \sum_{i=1}^k \sigma_i \mathbf u_i \mathbf v_i^T,<br>$$<br>then, for any matrix B of rank k , the minimal error is achieved with $A_k$:</p>$$\min_{\mathsf{rank}(B)=k}||A-B||_2 = || A - A_k||_2 = \sigma_{k+1}.$$<p>The same holds for Frobenius norm as well</p>$$\min_{\mathsf{rank}(B)=k}||A-B||_F = || A - A_k||_F = \sqrt{\sigma_{k+1}^2 + \cdots + \sigma_p^2}.$$<h3 id="证明-2-norm-case"><a href="#证明-2-norm-case" class="headerlink" title="证明 (2-norm case)"></a>证明 (2-norm case)</h3><p>Since $U^\ A_k V = \mathrm{diag}(\sigma_1,\ldots, \sigma_k,0,\ldots,0)$ it means that $A_k$ is rank K. Moreover, $U^T (A-A_k) V =  \mathrm{diag}(0,\ldots, 0,\sigma_{k+1},\ldots, \sigma_p)$with the largest singular value is $\sigma_{k+1}$ and thus $||A-A_k||_2 = \sigma_{k+1}$.</p><h3 id="证明-Frobenius-norm-case"><a href="#证明-Frobenius-norm-case" class="headerlink" title="证明 (Frobenius norm case)"></a>证明 (Frobenius norm case)</h3><blockquote><p>Lemma: If $A,B \in \mathbb{R}^{m\times n}$ , with B having rank K , then $\sigma_{k+i}(A) \le \sigma_i(A-B) \text{ for all }; i.$</p></blockquote><p>To prove the lemma, first consider the case i=1, we have proved that $\sigma_{k+1}(A) \le \sigma_1(A-B) = ||A-B||_2$in the 2-norm case. Then we do the general case:</p>$$\begin{aligned} \sigma_i(A-B) = &amp; \sigma_i(A-B) + \sigma_1(B-B_k)\qquad\text{since } B=B_k\\ =&amp; \sigma_1(A-B - (A-B)_{i-1}) + \sigma_1(B-B_k)\qquad\\ \ge &amp; \sigma_1(A-B - (A-B)_{i-1}+B-B_k)   \\ =&amp; \sigma_1(A  - (A-B)_{i-1} -B_k)\\ \ge &amp; \sigma_1(A - A_{k+i-1})\\ =&amp; \sigma_{k+i}(A)   \end{aligned} $$<blockquote><p><a href="https://zhuanlan.zhihu.com/p/361938622">https://zhuanlan.zhihu.com/p/361938622</a></p><p><a href="https://zhuanlan.zhihu.com/p/75283604">https://zhuanlan.zhihu.com/p/75283604</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> math </category>
          
          <category> 线性代数 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> math </tag>
            
            <tag> 线性代数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>低秩逼近的思考</title>
      <link href="/2021/08/06/low-rank-app/"/>
      <url>/2021/08/06/low-rank-app/</url>
      
        <content type="html"><![CDATA[<blockquote><p>阅读文章<strong>Semi-orthogonal Embedding for Effificient Unsupervised Anomaly Segmentation</strong>时出现一个概念–<strong>Low-rank approximation</strong> ，就此进行相关讨论。</p></blockquote><h3 id="低秩（Low-Rank）"><a href="#低秩（Low-Rank）" class="headerlink" title="低秩（Low-Rank）"></a>低秩（Low-Rank）</h3><p>如果X是一个m行n列的数值矩阵，rank(X)是X的秩，假如rank (X)远小于m和n，则我们称X是低秩矩阵。低秩矩阵每行或每列都可以用其他的行或列线性表出，可见它包含大量的冗余信息。利用这种冗余信息，可以对缺失数据进行恢复，也可以对数据进行特征提取。</p><p>图像处理中，<em>rank可以理解为图像所包含的信息的丰富程度</em>，在显示生活中，一张图片中大部分成分是相似的。比如给一张大草原的图片</p><p><img src="https://pic1.zhimg.com/50/ce29981e00f4d519ff547e986bf8a5d6_720w.jpg?source=1940ef5c"></p><p>草原是由很多草组成的，而草是相似的，所以如果全是草，那么这张图所包含的信息量是很少的的，因为可以理解为草是草的复制品。而上图的蒙古包，人，马之类的则可以理解为图片所包含的信息，实际上，相对于只有草的草原图片和有草和蒙古包的草原图片，后者的秩是较高的。也就是说，图片中比较突兀的成分，比如蒙古包，比如人像照片中的红眼亮点，会增加图像矩阵的秩。而现实生活中一张不错的图片的秩其实是比较低的，如果图像的秩比较高，往往是因为图像中的噪声比较严重。比如拍照的时候ISO感光度设置过高造成噪点太过泛滥之类的。所以，<em>图像处理的低秩性其实可以拿来去除照片中的噪点</em>。</p><h3 id="低秩和稀疏"><a href="#低秩和稀疏" class="headerlink" title="低秩和稀疏"></a>低秩和稀疏</h3><p>我们认为图像有一些公共的模式，所有图像都由这些基本的模式组成。例如，如果图像是一个叉，可以看成是一个正斜线和反斜线的叠加。<strong>只要我们找到了所有的基底（称作字典</strong>，就是上面说的正斜线和反斜线之类的东西）<strong>，就能通过基底的线性组合表示出所有的图像。</strong>这就好像学画画，先学会基本的画正方体、球体、圆柱体等等，就可以组合出各种各样的复杂形状。</p><p><strong>在很多情形下，基底的数量是很少的</strong>，比如一张照片拍的是一面砖墙，那么它显然具有周期重复的特点，换句话说低秩。即使整个图不低秩，往往也能找出一些相似的块，这些块是低秩的。再退一步，就算这也做不到，往往也可以把已有的数据看成一组低维的结果加上噪声，也即原来的数据<strong>可以被低秩矩阵很好的逼近</strong>。<strong>稀疏性</strong>的意思是（以稀疏表示为例），任给一个图像，<strong>字典可能是过完备的</strong>，从而用字典里的基向量表出这幅图有很多种不同的方案。我们希望<strong>选取使用基底数量最少的那种方案</strong>，</p><p>应用：</p><p><em><strong>1）矩阵填充(Matrix Completion)</strong></em></p><p><em><strong>2）鲁棒PCA</strong></em></p><p><em><strong>3）背景建模</strong></em></p><p><em><strong>4）变换不变低秩纹理（TILT）</strong></em></p><blockquote><p>在论文 <strong>Semi-orthogonal Embedding for Effificient Unsupervised Anomaly Segmentation</strong>中有一段可以参考。</p></blockquote><p><strong>Low-rank approximation of precision matrix</strong></p><p>The feature data <strong>X</strong> is subject to low-rank approximation due to the narrower target domain for anomaly-free images than the ImageNet dataset’s. The multi-scale features from different layers may also contribute to it due to the inter-dependency among the features from the layers. Inspired by the truncated SVD of a precision matrix, a low-rank embedding of input features with <strong>W</strong> <em>∈</em> $R^{F<em>k}$,where <em>F &gt; k</em>, is considered as follows:<br>$$<br>d^2_{i,j} = X^TW(W^TC_{i,j}W)^{−1}W^TX<br>$$<br>where the below Theorem 1 shows the optimal <strong>W</strong></em> is the eigenvectors related to the <em>k</em>-smallest eigenvalues of $C_{i,j}$ . Notice that 1) the computational complexity of the equation is cubically reduced to <em>O</em>($HWk^3$) set aside the cost of SVD, although which is the concern, 2) PCA embedding would fail to minimize approximation error since it uses the <em>k</em>-largest eigenvectors [14], and 3) near-zero eigenvalues may induce substantial anomaly scores.</p><p>选取协方差矩阵的k个最小的特征值对应的特征向量，进行低秩逼近</p><blockquote><p>参考</p><p> <a href="https://www.zhihu.com/question/28630628">https://www.zhihu.com/question/28630628</a></p><p><a href="https://blog.csdn.net/zouxy09/article/details/24972869">https://blog.csdn.net/zouxy09/article/details/24972869</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> math </category>
          
          <category> 线性代数 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> math </tag>
            
            <tag> 线性代数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>子模块为空的解决方案</title>
      <link href="/2021/08/06/git-submodule/"/>
      <url>/2021/08/06/git-submodule/</url>
      
        <content type="html"><![CDATA[<blockquote><p>针对子模块文件夹为空的情况，采取下列解决方案。</p><p>当一个 git 项目包含子模块（submodule) 时，直接克隆下来的子模块目录里面是空的。</p></blockquote><p><strong>有两种方法解决</strong>：</p><h3 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h3><p>如果项目已经克隆到了本地，执行下面的步骤：</p><ol><li><p>初始化本地子模块配置文件</p><figure class="highlight csharp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs csharp">git submodule <span class="hljs-keyword">init</span><br><span class="hljs-number">1</span><br></code></pre></td></tr></tbody></table></figure></li><li><p>更新项目，抓取子模块内容。</p><figure class="highlight ebnf"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">git submodule update</span><br></code></pre></td></tr></tbody></table></figure></li></ol><h3 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h3><p>另外一种更简单的方法，就是在执行 <code>git clone</code> 时加上 <code>--recursive</code> 参数。它会自动初始化并更新每一个子模块。例如：</p><figure class="highlight awk"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">git clone --recursive https:<span class="hljs-regexp">//gi</span>thub.com<span class="hljs-regexp">/example/</span>example.git<br></code></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo中公式显示</title>
      <link href="/2021/08/05/hexo-math-config/"/>
      <url>/2021/08/05/hexo-math-config/</url>
      
        <content type="html"><![CDATA[<blockquote><p>公式的显示问题。Hexo中Mathjax是用于显示公式的插件，但是多行显示会出现问题，有时还会出现乱码的情况。</p></blockquote><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>在根目录下的config_fluid.yml​文件中打开math的相关配置。</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># 数学公式，开启之前需要更换 Markdown 渲染器，否则复杂公式会有兼容问题，具体请见：https://hexo.fluid-dev.com/docs/guide/##latex-数学公式</span><br><span class="hljs-comment"># Mathematical formula. If enable, you need to change the Markdown renderer, see: https://hexo.fluid-dev.com/docs/en/guide/#math</span><br><span class="hljs-attr">math:</span><br>  <span class="hljs-comment"># 开启后文章默认可用，自定义页面如需使用，需在 Front-matter 中指定 `math: true`</span><br>  <span class="hljs-comment"># If you want to use math on the custom page, you need to set `math: true` in Front-matter</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br><br>  <span class="hljs-comment"># 开启后，只有在文章 Front-matter 里指定 `math: true` 才会在文章页启动公式转换，以便在页面不包含公式时提高加载速度</span><br>  <span class="hljs-comment"># If true, only set `math: true` in Front-matter will enable math, to load faster when the page does not contain math</span><br>  <span class="hljs-attr">specific:</span> <span class="hljs-literal">true</span><br><br>  <span class="hljs-comment"># Options: mathjax | katex</span><br>  <span class="hljs-attr">engine:</span> <span class="hljs-string">mathjax</span><br><br></code></pre></td></tr></tbody></table></figure><h3 id="出现的问题"><a href="#出现的问题" class="headerlink" title="出现的问题"></a>出现的问题</h3><h4 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h4><ul><li>由于hexo解码时关注，%% 等连续字符，会将这部分代码解读为其他带有特殊含义的内容</li><li>如果公式中恰巧出现了此类字符，会报出上述错误</li></ul><h4 id="问题2"><a href="#问题2" class="headerlink" title="问题2"></a>问题2</h4><ul><li>由于hexo在公式中的<code>\\</code>错会成了转义符，也就是说他只看见了一个反斜杠，不会执行换行命令，导致公式堆成一行</li></ul><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><h4 id="临时方案"><a href="#临时方案" class="headerlink" title="临时方案"></a>临时方案</h4><h5 id="针对问题1"><a href="#针对问题1" class="headerlink" title="针对问题1"></a>针对问题1</h5><ul><li>可以在连续的 <code>{</code> <code>}</code> <code>%</code>中间插入空格，分开就没事了</li></ul><h5 id="针对问题2"><a href="#针对问题2" class="headerlink" title="针对问题2"></a>针对问题2</h5><ul><li>可以将<code>\\</code>换成<code>\\\\</code>，可以实现公式的多行正确显示</li></ul><h4 id="终极方案"><a href="#终极方案" class="headerlink" title="终极方案"></a>终极方案</h4><ul><li><p>在官方文档中提到了可以为hexo提供标记，阻止其按照自己的规则解释我们的字符串，显示其原本的含义</p></li><li><p>标记为</p><figure class="highlight django"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs django"><span class="hljs-template-tag">{% <span class="hljs-name">raw</span> %}</span><span class="xml"></span><br><span class="xml">$$</span><br><span class="xml">...</span><br><span class="xml">$$</span><br><span class="xml"></span><span class="hljs-template-tag">{% <span class="hljs-name">endraw</span> %}</span><br></code></pre></td></tr></tbody></table></figure></li></ul><h3 id="多行显示和对齐"><a href="#多行显示和对齐" class="headerlink" title="多行显示和对齐"></a>多行显示和对齐</h3><ul><li><p>默认是显示为一行要实现公式多行和对齐可以使用{aligned}模式，使用”&amp;”来标记对齐位置。”\\“表示换行</p>  <figure class="highlight taggerscript"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs taggerscript">$$<br><span class="hljs-symbol">\b</span>egin{aligned}<br><span class="hljs-symbol">\b</span>oldsymbol{x}^{<span class="hljs-symbol">\m</span>athrm{T}}C<span class="hljs-symbol">\b</span>oldsymbol{x}&amp;=<span class="hljs-symbol">\b</span>oldsymbol{x}^{<span class="hljs-symbol">\m</span>athrm{T}}<span class="hljs-symbol">\m</span>athrm{E}<span class="hljs-symbol">\l</span>eft[<span class="hljs-symbol">\l</span>eft(X-<span class="hljs-symbol">\m</span>u<span class="hljs-symbol">\r</span>ight)<span class="hljs-symbol">\l</span>eft(X-<span class="hljs-symbol">\m</span>u<span class="hljs-symbol">\r</span>ight)^{<span class="hljs-symbol">\m</span>athrm{T}}<span class="hljs-symbol">\r</span>ight]<span class="hljs-symbol">\b</span>oldsymbol{x} <br><span class="hljs-symbol">\\</span>&amp;=<span class="hljs-symbol">\m</span>athrm{E}<span class="hljs-symbol">\l</span>eft[<span class="hljs-symbol">\b</span>oldsymbol{x}^{<span class="hljs-symbol">\m</span>athrm{T}}<span class="hljs-symbol">\l</span>eft(X-<span class="hljs-symbol">\m</span>u<span class="hljs-symbol">\r</span>ight)<span class="hljs-symbol">\l</span>eft(X-<span class="hljs-symbol">\m</span>u<span class="hljs-symbol">\r</span>ight)^{<span class="hljs-symbol">\m</span>athrm{T}}<span class="hljs-symbol">\b</span>oldsymbol{x}<span class="hljs-symbol">\r</span>ight] <span class="hljs-symbol">\\</span>&amp;=<span class="hljs-symbol">\m</span>athrm{E}<span class="hljs-symbol">\l</span>eft[<span class="hljs-symbol">\l</span>eft(<span class="hljs-symbol">\l</span>eft(X-<span class="hljs-symbol">\m</span>u<span class="hljs-symbol">\r</span>ight)^{<span class="hljs-symbol">\m</span>athrm{T}}<span class="hljs-symbol">\b</span>oldsymbol{x}<span class="hljs-symbol">\r</span>ight)^{<span class="hljs-symbol">\m</span>athrm{T}}<span class="hljs-symbol">\l</span>eft(<span class="hljs-symbol">\l</span>eft(X-<span class="hljs-symbol">\m</span>u<span class="hljs-symbol">\r</span>ight)^{<span class="hljs-symbol">\m</span>athrm{T}}<span class="hljs-symbol">\b</span>oldsymbol{x}<span class="hljs-symbol">\r</span>ight)<span class="hljs-symbol">\r</span>ight] <br><span class="hljs-symbol">\\</span>&amp;=<span class="hljs-symbol">\m</span>athrm{E}<span class="hljs-symbol">\l</span>eft(<span class="hljs-symbol">\l</span>eft<span class="hljs-symbol">\V</span>ert <span class="hljs-symbol">\l</span>eft(X-<span class="hljs-symbol">\m</span>u<span class="hljs-symbol">\r</span>ight)^{<span class="hljs-symbol">\m</span>athrm{T}}<span class="hljs-symbol">\b</span>oldsymbol{x}<span class="hljs-symbol">\r</span>ight<span class="hljs-symbol">\V</span>ert ^{2}<span class="hljs-symbol">\r</span>ight) <br><span class="hljs-symbol">\\</span>&amp;=<span class="hljs-symbol">\s</span>igma_{X}^{2}<br><span class="hljs-symbol">\e</span>nd{aligned}<br>$$<br></code></pre></td></tr></tbody></table></figure></li><li><p>显示为</p></li></ul>$$\begin{aligned}\boldsymbol{x}^{\mathrm{T}}C\boldsymbol{x}&amp;=\boldsymbol{x}^{\mathrm{T}}\mathrm{E}\left[\left(X-\mu\right)\left(X-\mu\right)^{\mathrm{T}}\right]\boldsymbol{x} \\&amp;=\mathrm{E}\left[\boldsymbol{x}^{\mathrm{T}}\left(X-\mu\right)\left(X-\mu\right)^{\mathrm{T}}\boldsymbol{x}\right] \\&amp;=\mathrm{E}\left[\left(\left(X-\mu\right)^{\mathrm{T}}\boldsymbol{x}\right)^{\mathrm{T}}\left(\left(X-\mu\right)^{\mathrm{T}}\boldsymbol{x}\right)\right] \\&amp;=\mathrm{E}\left(\left\Vert \left(X-\mu\right)^{\mathrm{T}}\boldsymbol{x}\right\Vert ^{2}\right) \\&amp;=\sigma_{X}^{2}\end{aligned}$$]]></content>
      
      
      <categories>
          
          <category> 配置 </category>
          
          <category> hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> fluid </tag>
            
            <tag> 配置 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>直观理解协方差矩阵</title>
      <link href="/2021/08/05/covariance-matrix/"/>
      <url>/2021/08/05/covariance-matrix/</url>
      
        <content type="html"><![CDATA[<blockquote><p>原文出自 <a href="https://zhuanlan.zhihu.com/p/349802953">https://zhuanlan.zhihu.com/p/349802953</a></p></blockquote><h2 id="1-概率论中的定义"><a href="#1-概率论中的定义" class="headerlink" title="1 概率论中的定义"></a>1 概率论中的定义</h2><h3 id="随机变量："><a href="#随机变量：" class="headerlink" title="随机变量："></a>随机变量：</h3><p>随机变量(Random Variable) X 是一个映射，把随机试验的结果与实数建立起了一一对应的关系。而期望与方差是随机变量的两个重要的数字特征。</p><h3 id="数学期望："><a href="#数学期望：" class="headerlink" title="数学期望："></a>数学期望：</h3><p>在概率论和统计学中，数学期望(mean)(或均值，亦简称期望(Expectation, or expected value))是试验中每次可能结果的概率乘以其结果的总和，是最基本的数学特征之一。它反映随机变量平均取值的大小。 期望值是该变量输出值的平均数。期望值并不一定包含于变量的输出值集合里。</p><p>大数定律规定，随着重复次数接近无穷大，数值的算术平均值几乎肯定地收敛于期望值。</p><h3 id="方差："><a href="#方差：" class="headerlink" title="方差："></a>方差：</h3><p>方差(Variance)是在概率论和统计方差衡量随机变量或一组数据时离散程度的度量。概率论中方差用来度量随机变量和其数学期望(即均值)之间的偏离程度。统计中的方差(样本方差)是每个样本值与全体样本值的平均数之差的平方值的平均数。</p><p>设$X$为随机变量， 如果$\mathrm{E}[X]$，则随机变量$X$的方差为：<br>$$<br>\mu=\mathrm{E}[X]<br>$$</p><p>方差也记为 $\sigma_{X}^{2}$。</p><p>样本方差计算公式：</p><p>$$<br>S^{2}=\Sigma\left(X-\overline{X}\right)^{2}/\left(n-1\right)<br>$$</p><p>其中，$S^{2}$为样本方差，$X$ 为变量，$\overline{X}$为样本均值，$n$ 为样本例数。如果要了解为什么要除以$n-1$，请看<a href="https://link.zhihu.com/?target=https://www.visiondummy.com/2014/03/divide-variance-n-1/">这篇文章</a>。</p><h3 id="标准差："><a href="#标准差：" class="headerlink" title="标准差："></a>标准差：</h3><p>标准差(Standard Deviation)是离均差平方的算术平均数(即：方差)的算术平方根，用$\sigma$表示。标准差也被称为标准偏差，或者实验标准差，在概率统计中最常使用作为统计分布程度上的测量依据。 见下图：</p><p><img src="https://pic1.zhimg.com/80/v2-37e04458baf17d09c914981b5dbae140_720w.jpg"></p><p>标准差是方差的算术平方根。标准差能反映一个数据集的离散程度。平均数相同的两组数据，标准差未必相同。</p><h3 id="协方差："><a href="#协方差：" class="headerlink" title="协方差："></a>协方差：</h3><p><strong>协方差(Covariance)在概率论和统计学中用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。</strong></p><p>期望值分别为$ E[X]$与$[Y]$的两个实随机变量$X$与$Y$之间的协方差 $\mathrm{Cov}(X,Y)$ 定义为：</p>$$\begin{aligned}\mathrm{Cov}(X,Y)&amp;=\mathrm{E}\left[\left(X\mathrm{E}\left[X\right]\right)\left(Y-\mathrm{E}\left[Y\right]\right)\right]  \\&amp;=\mathrm{E}\left[XY\right]-2\mathrm{E}\left[Y\right]\mathrm{E}\left[X\right]+\mathrm{E}\left[X\right]\mathrm{E}\left[Y\right]\\&amp;=\mathrm{E}\left[XY\right]-\mathrm{E}\left[X\right]\mathrm{E}\left[Y\right] \\&amp;=\mathrm{E}\left[XY\right]-\mathrm{E}\left[X\right]\mathrm{E}\left[Y\right]\end{aligned}$$<p>协方差表示的是两个变量总体误差的期望。 如果两个变量的变化趋势一致，也就是说如果其中一个大于自身的期望值，另外一个也大于自身的期望值，那么两个变量之间的协方差就是正值。 如果两个变量的变化趋势相反，即其中一个大于自身的期望值，另外一个却小于自身的期望值，那么两个变量之间的协方差就是负值。</p><p>如果$X$与$Y$是统计独立的，那么二者之间的协方差就是0，因为两个独立的随机变量满足$\mathrm{E}[XY]=\mathrm{E}[X]\mathrm{E}[Y]$。但是，反过来并不成立。即如果$X$与$Y$的协方差为0，二者并不一定是统计独立的。</p><p><strong>协方差为0的两个随机变量称为是不相关的。</strong></p><h3 id="协方差矩阵："><a href="#协方差矩阵：" class="headerlink" title="协方差矩阵："></a>协方差矩阵：</h3><p>在统计学与概率论中，协方差矩阵(Covariance matrix)的每个元素是各个向量元素之间的协方差，是从标量随机变量到高维度随机向量的自然推广。</p><p>设$X=\left(X_{1},X_{2},\ldots,X_{n}\right)^{\mathrm{T}}$为$n$ 维随机变量，称矩阵</p>$$C=\left(\begin{array}{cccc} c_{11} &amp; c_{12} &amp; \cdots &amp; c_{1n}\\ c_{21} &amp; c_{22} &amp; \cdots &amp; c_{2n}\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\ c_{n1} &amp; c_{n2} &amp; \cdots &amp; c_{nn} \end{array}\right)$$<p>为 n 维随机变量x的协方差矩阵，也记为 $D\left(X\right)$ ，其中<br>$$<br>c_{ij}=\mathrm{Cov}(X_{i},X_{j}),\quad i,j=1,2,\ldots,n<br>$$<br>为X的分量$X_{i}$和$X_{j}$的协方差。<em>并且对角线上的元素为各个随机变量的方差：</em></p><p>$$<br>c_{ii}=\mathrm{Cov}(X_{i},X_{i}),\quad i=1,2,\ldots,n<br>$$</p><p>协方差矩阵是对称半正定矩阵。协方差矩阵的对称性，可从定义得知。对于半正定特性，证明如下：</p><p>现给定任意一个非零向量$\boldsymbol{x}$，则</p>$$\begin{aligned}\boldsymbol{x}^{\mathrm{T}}C\boldsymbol{x}&amp;=\boldsymbol{x}^{\mathrm{T}}\mathrm{E}\left[\left(X-\mu\right)\left(X-\mu\right)^{\mathrm{T}}\right]\boldsymbol{x} \\&amp;=\mathrm{E}\left[\boldsymbol{x}^{\mathrm{T}}\left(X-\mu\right)\left(X-\mu\right)^{\mathrm{T}}\boldsymbol{x}\right] \\&amp;=\mathrm{E}\left[\left(\left(X-\mu\right)^{\mathrm{T}}\boldsymbol{x}\right)^{\mathrm{T}}\left(\left(X-\mu\right)^{\mathrm{T}}\boldsymbol{x}\right)\right] \\&amp;=\mathrm{E}\left(\left\Vert \left(X-\mu\right)^{\mathrm{T}}\boldsymbol{x}\right\Vert ^{2}\right) \\&amp;=\sigma_{X}^{2}\end{aligned}$$<p>其中，<br>$$<br>\sigma_{X}=\left(X-\mu \right)^{\mathrm{T}}\boldsymbol{x}<br>$$<br>由于 $\sigma_{X}^{2}\geq0$，因此$\boldsymbol{x}^{\mathrm{T}}C\boldsymbol{x}\geq0$，因此协方差矩阵$C$ 是半正定矩阵。</p>]]></content>
      
      
      <categories>
          
          <category> math </category>
          
          <category> 概率论 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> math </tag>
            
            <tag> 概率论 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
